{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stateful LSTM for a One-Char to One-Char Mapping\n",
    "\n",
    "We have seen that we can break-up our raw data into fixed size sequences and that this representation can be learned by the LSTM, but only to learn random mappings of 3 characters to 1 character.\n",
    "\n",
    "We have also seen that we can pervert batch size to offer more sequence to the network, but only during training.\n",
    "\n",
    "Ideally, *we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem.*\n",
    "\n",
    "**We can do this in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.**\n",
    "\n",
    "**This is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).**\n",
    "\n",
    "*为什么要**reset**, If the model is stateless, the cell states are reset at each sequence. With the stateful model, **all the states are propagated to the next batch**. It means that the state of the sample located at index i, Xi will be used in the computation of the sample Xi+bs in the next batch, where bs is the batch size (no shuffling).*\n",
    "\n",
    "We first need to define our LSTM layer as stateful. **In so doing, we must explicitly specify the batch size as a dimension on the input shape. This also means that when we evaluate the network or make predictions, we must also specify and adhere to this same batch size. **This is not a problem now as we are using a batch size of 1. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.\n",
    "\n",
    "    batch_size = 1\n",
    "    model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    \n",
    "An important difference in training the stateful LSTM is that we train it manually one epoch at a time and reset the state after each epoch. We can do this in a for loop. Again, we do not shuffle the input, preserving the sequence in which the input training data was created.\n",
    "\n",
    "    for i in range(300):\n",
    "\tmodel.fit(X, y, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "    \n",
    "As mentioned, we specify the batch size when evaluating the performance of the network on the entire training dataset.  \n",
    "\n",
    "    # summarize performance of the model\n",
    "    scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "    model.reset_states()\n",
    "    print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "Finally, we can demonstrate that the network has indeed learned the entire alphabet. We can seed it with the first letter “A”, request a prediction, feed the prediction back in as an input, and repeat the process all the way to “Z”.\n",
    "\n",
    "\n",
    "    # demonstrate some model predictions\n",
    "    seed = [char_to_int[alphabet[0]]]\n",
    "    for i in range(0, len(alphabet)-1):\n",
    "        x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "        x = x / float(len(alphabet))\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "        seed = [index]\n",
    "    model.reset_states()\n",
    "\n",
    "We can also see if the network can make predictions starting from an arbitrary letter.\n",
    "\n",
    "    # demonstrate a random starting point\n",
    "    letter = \"K\"\n",
    "    seed = [char_to_int[letter]]\n",
    "    print \"New start: \", letter\n",
    "    for i in range(0, 5):\n",
    "        x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "        x = x / float(len(alphabet))\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "        seed = [index]\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(batch_size, X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "这个形式看着跟(1,1,1) 1对1训练的类似，但是由于用了stateful, 上一次训练的结果被保留，知道整个X 被用完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "0s - loss: 3.2659 - acc: 0.0400\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2538 - acc: 0.1600\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2479 - acc: 0.1600\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2422 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2364 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2302 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2234 - acc: 0.0400\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2156 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.2064 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.1950 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.1806 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.1615 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.1359 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.1030 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0666 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0357 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0185 - acc: 0.1600\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0204 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0309 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 3.0175 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.9810 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.9444 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.9111 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.8777 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.8444 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.8103 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.7749 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.7385 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.7026 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.6682 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.6334 - acc: 0.1600\n",
      "Epoch 1/1\n",
      "0s - loss: 2.6004 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.5698 - acc: 0.2400\n",
      "Epoch 1/1\n",
      "0s - loss: 2.5453 - acc: 0.2400\n",
      "Epoch 1/1\n",
      "0s - loss: 2.5261 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.5061 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4833 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4561 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4180 - acc: 0.2800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4078 - acc: 0.2400\n",
      "Epoch 1/1\n",
      "0s - loss: 2.3635 - acc: 0.2800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.3871 - acc: 0.2400\n",
      "Epoch 1/1\n",
      "0s - loss: 2.2858 - acc: 0.2800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4272 - acc: 0.1600\n",
      "Epoch 1/1\n",
      "0s - loss: 2.2324 - acc: 0.2800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.5700 - acc: 0.1200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.2319 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "0s - loss: 2.4644 - acc: 0.0800\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1735 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.2145 - acc: 0.2400\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1804 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1530 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1587 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1444 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1357 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1245 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.1137 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0982 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0754 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0581 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0392 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0213 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 2.0097 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9914 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9797 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9674 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9547 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9480 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9363 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9262 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9141 - acc: 0.3200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.9053 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8989 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8817 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8683 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8607 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8611 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8613 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8514 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8474 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8308 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8195 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8084 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8118 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.8124 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7965 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7918 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7877 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7850 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7792 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7674 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7554 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7434 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7338 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7267 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7169 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7095 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.7047 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6909 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6829 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6752 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6671 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6596 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6536 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6464 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6418 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6361 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6318 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6286 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6237 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6188 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6121 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6065 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.6017 - acc: 0.3600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5913 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5786 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5720 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5654 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5623 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5575 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5532 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5482 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5387 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5305 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5234 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5155 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5097 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.5023 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4984 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4910 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4834 - acc: 0.4400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4778 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4720 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4643 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4599 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4527 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4459 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4409 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4315 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4230 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4158 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4084 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.4028 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3991 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3960 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3908 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3872 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3826 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3760 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3717 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3681 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3630 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3590 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3538 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3515 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3482 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3448 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3423 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3384 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3373 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3337 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3277 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3257 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3245 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3219 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3145 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3125 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3135 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3113 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3036 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3014 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.3019 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2994 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2956 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2936 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2904 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2886 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2855 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2850 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2839 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2826 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2823 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2832 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2828 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2812 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2779 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2764 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2759 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2737 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2710 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2695 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2646 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2613 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2599 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2577 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2568 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2526 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2479 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2447 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2410 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2408 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2396 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2366 - acc: 0.4800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2320 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2293 - acc: 0.5200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2275 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2284 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2274 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2248 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2203 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2171 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2183 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2167 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2135 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2118 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2074 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2084 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2054 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2023 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.2001 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1985 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1991 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1941 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1937 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1915 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1904 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1909 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1873 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1843 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1827 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1789 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1817 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1811 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1739 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1702 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1678 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1634 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1568 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1548 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1517 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1485 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1490 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1444 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1413 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1472 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1417 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1390 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1358 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1341 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1343 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1243 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1259 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1285 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1244 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1183 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1153 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1182 - acc: 0.7600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1146 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1139 - acc: 0.7600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1056 - acc: 0.7600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1063 - acc: 0.7600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1050 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1043 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1073 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1079 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1024 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0971 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0919 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0982 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1094 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1002 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1045 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0989 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0980 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0943 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0936 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0819 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0909 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0890 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0924 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0826 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0766 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0756 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0772 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0768 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0798 - acc: 0.6400\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0870 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0845 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0806 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0849 - acc: 0.5600\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0759 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0757 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0725 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0756 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0786 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0830 - acc: 0.6000\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0817 - acc: 0.6800\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0845 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.0935 - acc: 0.7200\n",
      "Epoch 1/1\n",
      "0s - loss: 1.1007 - acc: 0.7200\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 72.00%\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> E\n",
      "E -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> J\n",
      "J -> L\n",
      "L -> M\n",
      "M -> M\n",
      "M -> N\n",
      "N -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> S\n",
      "S -> U\n",
      "U -> V\n",
      "V -> V\n",
      "V -> W\n",
      "W -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New start:  K\n",
      "K -> B\n",
      "B -> C\n",
      "C -> C\n",
      "C -> D\n",
      "D -> E\n"
     ]
    }
   ],
   "source": [
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print(\"New start: \", letter)\n",
    "for i in range(0, 5):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that the network has memorized the entire alphabet perfectly. It used the context of the samples themselves and learned whatever dependency it needed to predict the next character in the sequence.\n",
    "\n",
    "We can also see that if we seed the network with the first letter, that it can correctly rattle off the rest of the alphabet.\n",
    "\n",
    "We can also see that it has only learned the full alphabet sequence and that from a cold start. When asked to predict the next letter from “K” that it predicts “B” and falls back into regurgitating the entire alphabet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
